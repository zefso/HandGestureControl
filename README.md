# ğŸ–ï¸ Hand Gesture Control System

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![License](https://img.shields.io/badge/License-MIT-green)
![Status](https://img.shields.io/badge/Status-Active-success)

[Read in Ukrainian UA](README.uk.md)

Welcome to the **Hand Gesture Control System**! ğŸ–±ï¸ğŸ‘‹
This project empowers you to control your computer mouse and execute commands using simple hand gestures. Powered by **Computer Vision** and **Deep Learning**, it offers a touch-free interface experience.

Built with powerful technologies:
-   **MediaPipe** for ultra-fast hand tracking.
-   **LSTM (Long Short-Term Memory)** neural networks for accurate gesture recognition.
-   **OpenCV** for image processing.

---

## ğŸ“‘ Table of Contents

-   [âœ¨ Features](#-features)
-   [ğŸ“‚ Project Structure](#-project-structure)
-   [ğŸš€ Installation](#-installation)
    -   [Prerequisites](#prerequisites)
    -   [Setup Steps](#setup-steps)
-   [ğŸ•¹ï¸ Usage](#ï¸-usage)
    -   [1. Data Collection](#1-data-collection-optional)
    -   [2. Model Training](#2-model-training)
    -   [3. Running the System](#3-running-the-system)
-   [âš™ï¸ Configuration](#ï¸-configuration)
-   [â“ Troubleshooting](#-troubleshooting)

---

## âœ¨ Features

-   **ğŸ–ï¸ Real-time Tracking**: Instantly detects and tracks hand landmarks with high precision.
-   **ğŸ§  Smart Recognition**: Uses an LSTM model to understand dynamic gestures (motion sequences), not just static poses.
-   **ğŸ–±ï¸ Mouse Control**: Move the cursor, click, drag, and scroll using natural hand movements.
-   **âš¡ Low Latency**: Optimized for smooth performance on standard CPUs.
-   **ğŸ¨ Custom Gestures**: Comes with pre-train gestures, but you can easily add your own!
    -   `static` ğŸ‹ï¸ (No action)
    -   `swipe_right` â¡ï¸ (Next window/tab)
    -   `swipe_left` â¬…ï¸ (Previous window/tab)
    -   `ok` ğŸ‘Œ (Confirmation/Click)
    -   `stop` ğŸ¤š (Pause/Hold)
    -   `browser` âœŒï¸ (Open Browser)
    -   `fist_left` âœŠ (Grab/Drag)

---

## ğŸ“‚ Project Structure

Here's how the project is organized:

```text
HandGestureControl/
â”œâ”€â”€ ğŸ“ models/             # Contains trained model files (.pth)
â”œâ”€â”€ ğŸ“ src/                # Main source code directory
â”‚   â”œâ”€â”€ ğŸ“„ config.py       # Central configuration (gestures, paths, params)
â”‚   â”œâ”€â”€ ğŸ“„ main.py         # ğŸš€ Main entry point to run the controller
â”‚   â”œâ”€â”€ ğŸ“„ model.py        # ğŸ§  Neural Network architecture (LSTM)
â”‚   â”œâ”€â”€ ğŸ“„ train.py        # ğŸ‹ï¸ Script to train the model
â”‚   â”œâ”€â”€ ğŸ“„ smart_collector.py # ğŸ“· Tool for recording new gestures
â”‚   â”œâ”€â”€ ğŸ“„ mouse_controller.py # ğŸ–±ï¸ Logic for mouse interaction
â”‚   â””â”€â”€ ğŸ“„ utils.py        # ğŸ› ï¸ Helper functions
â”œâ”€â”€ ğŸ“ data/               # Raw training data (sequences of numpy arrays)
â”œâ”€â”€ ğŸ“ diagram/            # ğŸ“Š Training performance graphs
â”œâ”€â”€ ğŸ“„ requirements.txt    # Python dependencies
â””â”€â”€ ğŸ“„ README.md           # This documentation
```

---

## ğŸš€ Installation

### Prerequisites

-   **Python 3.8** or higher installed.
-   A working **Webcam**.
-   **Git** (optional, for cloning).

### Setup Steps

1.  **Clone the Repository**
    Open your terminal or command prompt and run:
    ```bash
    git clone <repository_url>
    cd HandGestureControl
    ```

2.  **Create a Virtual Environment** (Recommended)
    It's best to keep dependencies isolated.

    *   **Windows:**
        ```bash
        python -m venv venv
        venv\Scripts\activate
        ```
    *   **macOS / Linux:**
        ```bash
        python3 -m venv venv
        source venv/bin/activate
        ```

3.  **Install Dependencies**
    Install all required libraries using `pip`:
    ```bash
    pip install -r requirements.txt
    ```
    > **Note:** Major libraries include `opencv-python`, `mediapipe`, `torch`, `pyautogui`, `scikit-learn`.

---

## ğŸ•¹ï¸ Usage

### 1. Data Collection (Optional)
*Skip this if you just want to use the pre-trained model.*
To train the system on **your** specific hand movements:
```bash
python src/smart_collector.py
```
-   Follow the on-screen prompts.
-   Press `n` to switch gestures.
-   Press `Space` to record a sequence.
-   Data is saved to `data/<gesture_name>/`.

### 2. Model Training
After collecting data (or if you added new gestures), retrain the brain:
```bash
python src/train.py
```
-   The script automatically loads data, trains the LSTM, and validates accuracy.
-   The best model is saved as `models/gesture_lstm_best.pth`.
-   Check `diagram/` for accuracy/loss graphs! ğŸ“ˆ

### 3. Running the System
Ready to control your mouse?
```bash
python src/main.py
```
-   **Controls**:
    -   **Move Mouse**: Point with your index finger.
    -   **Left Click**: Pinch Index + Thumb.
    -   **Right Click**: Pinch Middle + Thumb.
    -   **Scroll**: Raise Index + Middle fingers together.
-   **Exit**: Press `q` to quit the application.

---

## âš™ï¸ Configuration

Tweak the system in `src/config.py`:

| Variable | Description | Default |
| :--- | :--- | :--- |
| `GESTURES` | List of gesture names to recognize | `['static', 'swipe_right', ...]` |
| `SEQ_LENGTH` | Number of frames per gesture sequence | `30` |
| `THRESHOLD` | Confidence required to trigger action | `0.92` (92%) |
| `EPOCHS` | Training iterations | `150` |

---

## â“ Troubleshooting

**Q: The system doesn't detect my hands.**
> **A:** Ensure lighting is good. Avoid backlighting. Keep your hands within the camera frame.

**Q: Mouse movement is jittery.**
> **A:** Increase the `smoothing` parameter in `mouse_controller.py` or adjust lighting.

**Q: Error: `Module not found`.**
> **A:** Make sure you activated your virtual environment and ran `pip install -r requirements.txt`.

**Q: Gestures are mixed up.**
> **A:** Try retraining the model with your own data using `smart_collector.py`. Everyone's hands move differently!

---

*Created with â¤ï¸ for seamless Human-Computer Interaction.*
